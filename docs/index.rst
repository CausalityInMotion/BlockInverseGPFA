=======================================
Gaussian-Process Factor Analysis (GPFA)
=======================================

Gaussian-process factor analysis (GPFA) is a dimensionality reduction method [Yu2009]_ that extracts smooth, low-dimensional latent trajectories from noisy, high-dimensional time series data.
GPFA applies `factor analysis (FA) <https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.FactorAnalysis.html>`_ to observed data to simultaneously reduce their dimensionality and smooth the resulting low-dimensional trajectories by fitting a `Gaussian process (GP) <https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.Kernel.html#sklearn.gaussian_process.kernels.Kernel>`_ model to them.
See the below :ref:`notes` for its main differences to the popular PCA dimensionality redunction method.

The essential ingredients of the generative model underlying GPFA are *observations* :math:`\boldsymbol{X}` and *latent variables* :math:`\boldsymbol{Z}`.
For each high-dimensional time series in a dataset, the :math:`x_\textrm{dim}`-dimensional time series is split into :math:`T` time bins of size :math:`\delta t`, leading to the :math:`x_\textrm{dim} \times T`  matrix :math:`\boldsymbol{X}` that contains these data.
The observations in each time bin are assumed to be stochastically generated by a :math:`z_\textrm{dim}`-dimensional latent variable (:math:`z_\textrm{dim} < x_\textrm{dim}`) through some noisy linear mapping.
Across time, these latent variables are collected in the :math:`z_\textrm{dim} \times T` matrix :math:`\boldsymbol{Z}`.
A dataset usually contains a set of observations :math:`\boldsymbol{X}`'s that each have their associated latent variabes :math:`\boldsymbol{Z}`, and that all have the same observation and latent dimensions, :math:`x_\textrm{dim}` and :math:`z_\textrm{dim}`, but might differ in their number of time bins :math:`T`. 

GPFA's generative model is defined by the emission and the latent time-course models:

1. **Emission model**, :math:`p(\boldsymbol{X} | \boldsymbol{Z})`: GPFA's emission model assumes that, in each time bin :math:`t`, the observations :math:`\boldsymbol{x}_{:,t}` are generated independently through a stochastic linear mapping from the corresponding latent variables :math:`\boldsymbol{z}_{:,t}`,

   .. math:: \boldsymbol{x}_{:,t} | \boldsymbol{z}_{:,t} \sim \mathcal{N} \left( \boldsymbol{C} \boldsymbol{z}_{:,t} + \boldsymbol{d}, \boldsymbol{R} \right) ,

   where :math:`\boldsymbol{x}_{:,t}` and :math:`\boldsymbol{z}_{:,t}` are the :math:`t` th column (i.e., the :math:`t` th time bin) of :math:`\boldsymbol{X}` and :math:`\boldsymbol{Z}`, respectively, :math:`\mathcal{N}\left( \boldsymbol{\mu}, \boldsymbol{\Sigma}\right)` denotes the multivariate Gaussian with mean :math:`\boldsymbol{\mu}` and covariance :math:`\boldsymbol{\Sigma}`, and the loading matrix :math:`\boldsymbol{C}` (:math:`x_\textrm{dim} \times z_\textrm{dim}`), bias vector :math:`\boldsymbol{d}` (:math:`x_\textrm{dim}`), and diagonal (as in FA) emission covariance matrix :math:`\boldsymbol{R}` (:math:`x_\textrm{dim} \times x_\textrm{dim}`) are model parameters.

2. **Latent time-course model**, :math:`p(\boldsymbol{Z})`: GPFA assumes that the latent variables evolve in each latent dimension :math:`i` (or *factor*) independently according to a Gaussian process,

   .. math:: \boldsymbol{z}_{i,:} \sim \mathcal{GP} \left( \boldsymbol{0}, \boldsymbol{K} \right) ,

   where :math:`\boldsymbol{z}_{i,:}` is the :math:`i` th row (i.e., :math:`i` th latent dimension/factor) of :math:`\boldsymbol{Z}`, and :math:`\mathcal{GP} \left( \boldsymbol{0}, \boldsymbol{K} \right)` denotes a zero-mean Gaussian process with covariance kernel :math:`\boldsymbol{K}`. 

Overall, the model has emission model parameters :math:`\boldsymbol{C}`, :math:`\boldsymbol{d}`, and :math:`\boldsymbol{R}` and time-course model parameters that are the parameters of the chosen GP kernel :math:`\boldsymbol{K}`. When GPFA is fit to a set of observations, :math:`\boldsymbol{X}_1, \boldsymbol{X}_2, \dots`, GPFA adjusts these parameters and associated latent variables to best explain these observations.


.. _notes:

Notes
=====
.. These are some of the things worthy of note, but I think they are better
.. fitted in the GPFA class documentation (we can discuss this).
.. - Differences between the original implementation and the current one:
  
..   - Using :func:`use_cut_trials`, pros and cons
..   - block inverse computation
..   - GP Kernel
..   - Variance explained

GPFA differs from the popular `PCA <https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html>`_ in two significant ways:

1. It performs dimensionality reduction by factor analysis, which supports different residual variances in different data dimensions, whereas PCA assumed this residual variance to be the same across all dimensions.
2. PCA does not assume any temperal dependencies between different latent variables.
   GPFA, in contrast assumes latent trajecories to be smooth across time, and models this assumption by a Gaussian process.

.. _examples:

Examples (TODO)
===============
.. Discuss appropriate examples

>>> import numpy as np
>>> from gpfa import GPFA
>>> from sklearn.gaussian_process.kernels import RBF, WhiteKernel
>>> from sklearn.gaussian_process.kernels import ConstantKernel

>>> # set random parameters
>>> seed = [0, 8, 10]
>>> z_dim = 3
>>> units = 10
>>> tau_f = 0.1
>>> sigma_n = 0.001
>>> sigma_f = 1 - sigma_n
>>> bin_size = 0.02  # [s]
>>> num_trials = 3
>>> n_timesteps = 500
>>> kernel = ConstantKernel(
...                    sigma_f, constant_value_bounds='fixed'
...                    ) * RBF(length_scale=tau_f) + ConstantKernel(
...                    sigma_n, constant_value_bounds='fixed'
...                    ) * WhiteKernel(
...                        noise_level=1, noise_level_bounds='fixed'
...                    )
>>> tsdt = np.arange(0, n_timesteps) * bin_size
>>> mu = np.zeros(tsdt.shape)
>>> cov = kernel(tsdt[:, np.newaxis])
>>> C = np.random.uniform(0, 2, (units, z_dim))     # loading matrix
>>> obs_noise = np.random.uniform(0.2, 0.75, units) # rand noise parameters
>>> X = []
>>> for n in range(num_trials):
>>>     np.random.seed(seed[n])
>>>     # Draw three latent state samples from a Gaussian process
>>>     # using the above cov
>>>     Z = np.random.multivariate_normal(mu.ravel(), cov, z_dim)
>>>     # observations have Gaussian noise
>>>     x = C @ Z + np.random.normal(0, obs_noise, (n_timesteps, units)).T
>>>     X.append(x)
>>> gpfa = GPFA(bin_size=bin_size, z_dim=z_dim)
>>> gpfa.fit(X)
Initializing parameters using factor analysis...
Fitting GPFA model...
>>> results, _ = gpfa.predict(returned_data=['pZ_mu', 'pZ_mu_orth'])
>>> pZ_mu_orth = results['pZ_mu_orth']
>>> pZ_mu = results['pZ_mu']
>>> gpfa.variance_explained()
(0.93590..., array([0.76541..., 0.10446..., 0.066033...]))

>>> # GPFA on synthetic spike data
>>> import numpy as np
>>> from gpfa import GPFA
>>> from gpfa.preprocessing import EventTimesToCounts
>>> from sklearn.preprocessing import FunctionTransformer
>>> seed = [0, 8, 10, 42, 60]
>>> rate = 50
>>> units = 10
>>> durations = [500, 550, 600, 650, 700]  # [ms]
>>> num_trials = len(durations)
>>> X = np.zeros(num_trials, object)
>>> for i in range(num_trials):
>>>     np.random.seed(seed[i])
>>>     Data[i] = np.random.poisson(rate, (units, durations[i]))
>>> event_times_to_counts = EventTimesToCounts(extrapolate_last_bin=True)
>>> binned_spiketrians = [
...    event_times_to_counts.transform(x_i) for x_i in X
...    ]
>>> fun_trans = FunctionTransformer(np.sqrt)
>>> sqrt_spike_trains = [
...    fun_trans.transform(x_i) for x_i in binned_spiketrians
...    ]
>>> z_dim = 3
>>> bin_size = 0.02  # [s]
>>> gpfa = GPFA(bin_size=bin_size, z_dim=z_dim, em_max_iters=2)
>>> gpfa.fit(X)
Initializing parameters using factor analysis...
Fitting GPFA model...
>>> results, _ = gpfa.predict(returned_data=['pZ_mu','pZ_mu_orth'])
>>> pZ_mu_orth = results['pZ_mu_orth']
>>> pZ_mu = results['pZ_mu']
>>> gpfa.variance_explained()
(0.98518..., array([9.85162126e-01, 1.32456401e-05, 7.66699002e-06]))

Original code
-------------
The code was adjusted and extended from the `Elephant ephys analysis toolkit <http://neuralensemble.org/elephant/>`_ which is a direct Python translation of `Byron Yu's MATLAB implementation <https://users.ece.cmu.edu/~byronyu/software.shtml>`_.

References
----------
.. [Yu2009] `Yu, Byron M and Cunningham, John P and Santhanam, Gopal and
    Ryu, Stephen and Shenoy, Krishna V and Sahani, Maneesh
    "Gaussian-process factor analysis for low-dimensional single-trial
    analysis of neural population activity"
    In Journal of Neurophysiology, Vol. 102, Issue 1. pp. 614-635.
    <https://doi.org/10.1152/jn.90941.2008>`_

.. [RW2006] `Carl E. Rasmussen and Christopher K.I. Williams,
   "Gaussian Processes for Machine Learning",
   MIT Press 2006 <https://www.gaussianprocess.org/gpml/chapters/RW.pdf>`_

.. _main_page:
.. toctree::
    :maxdepth: 2

    gpfa_class
    preprocessing_class
    installation
    contribute
    acknowledgments
    authors
    citation
